{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Injection\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'text.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Loading\n",
    "loader = TextLoader(\"text.txt\")\n",
    "text_document = loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loading\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "pdf_document = loader.load()\n",
    "pdf_document[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'http://www.plaintxt.org/', 'title': 'plaintxt.org – Minimalism in blog design, an experiment', 'description': 'Plaintxt.org was the first Web site dedicated to minimalism in blog design and the original home of the Sandbox theme for WordPress.', 'language': 'No language found.'}, page_content='\\n\\n\\nplaintxt.org – Minimalism in blog design, an experiment\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nplaintxt.org\\nMinimalism in blog design, an experiment\\n\\n\\n\\nContents\\n\\nIntroduction\\nThemes\\nExperiments (Plugins)\\nMiscellaneous\\nLicense\\nTerms of use\\n\\n\\n\\n\\n\\nIntroduction\\nOnce upon a time, I was an actively developing themes and plugins for WordPress. No more. No, I have since moved on to other projects. And while this site is no longer updated, I want it to remain because when I was developing for WordPress, I found referencing other themes and plugins useful, even necessary.\\nBut with the inclusion of the three functions that were the heart and soul of the Sandbox now in the WordPress core, my mark has been made, and there are new directions to go. Those functions, sandbox_body_class(), sandbox_post_class(), and sandbox_comment_class(), can be found in your recent WordPress download, simply renamed as body_class(), post_class(), and comment_class(), respectively. Mission accomplished.\\nSo I hope what I did contribute once upon a time will continue to be of some use to the community. Plaintxt.org has become a single-page archive of what it was. Enjoy.\\n\\nOriginal Welcome\\nMake yourself comfortable. Welcome to my personal experiment in combining minimalism with blog design. I offer themes that showcase your content and not my design skills, which are meager enough. Blogging shouldn’t be about filling a template.\\nThat’s how plaintxt.org came to be: a home for minimalism in blog design. The designs here are solutions to problems that I encountered. These are WordPress themes that addressed my needs and tastes. I hope you find them useful too.\\nThese themes include “premium” features, e.g., theme option menus that allow almost complete personalization as well as total Widgets compatibility, in addition to style sheets (including print styles) that demonstrate the highest attention to detail.\\nEach theme is free (as in speech) and licensed with the GNU General Public License, compatible with WordPress, a free semantic publishing platform, and include neither sponsored links nor deceitful code.\\nSandbox Designs Competition\\nPerhaps you were redirected here from sndbx.org. Well, back in 2007, I held a design competition for theme templates based on the Sandbox. This was the first such design competition in the WordPress community, and it went off without a hitch. Sponsors included Press Harbor, WordPress for Dummies, Matt, and others. Cash awards were given. It was awesome.\\nOddly, the most frequent request I have regarding the Sandbox Designs Competition is for the example content I provided so contestants could see exactly how their theme would look with the very content used in judging (the file, sndbx.2007-06-06.xml, is still available). \\nIf you are interested in any of the particulars of the Sandbox Designs Competition, I archive the sndbx.org content (now offline) in a PDF, Sandbox Designs Competition (Revision 5). Enjoy.\\nBack to contents\\n\\n\\nThemes\\nThese free themes are designed for use with WordPress, a free semantic publishing platform, and are licensed with the GNU General Public License. These themes contain no sponsored links or any malicious code, are Widgets compatible, and include theme options submenus.\\nBy downloading content, you agree to the Terms of use.\\n\\n\\nBarthelme\\n\\nMy favorite theme: Two columns with an unique approach to the fluid layout and typographically rich. An excellent choice for a photoblog, vlog, or any other content-centered blog. Features include customizable theme options from within the dashboard.\\nDownload Barthelme template\\nView the Barthelme README\\nLive preview the Barthelme template\\n\\n\\nblog.txt\\n\\nAn elastic layout that is minimal: Less than 300 lines of CSS for screen and print. Robust theme options provide two- or three-column layouts for nine possible combinations. Named a top WordPress theme in Blogging for Dummies. A quintessential WordPress theme.\\nDownload blog.txt template\\nView the blog.txt README\\nLive preview the blog.txt template\\n\\n\\nplaintxtBlog\\n\\nThe original minimalist three-column theme for WordPress. Its uncluttered layout make it deal for customization, with a wide content area to ensure your content is what visitors notice, the theme itself transparent. Sidebars are intentionally narrow to discourage clutter.\\nDownload plaintxtBlog template\\nView the plaintxtBlog README\\nLive preview the plaintxtBlog template\\n\\n\\nSandbox\\n\\nOne of the most influential blog themes, the Sandbox is a starting point for designers and developers—the original and best blank slate theme. The Sandbox is rich with semantic classes powered by dynamic functions and Microformats. A collaboration between Scott (that’s me) and Andy.\\nDownload Sandbox template\\nView the Sandbox README\\nLive preview the Sandbox template\\n\\n\\nSimplr\\n\\nThe original minimalist one-column theme for WordPress. It provides a unique format for highlighting the latest content. This is the theme I always wanted: one to reduce clutter and to provide focus. Features built-in recent comments and breadcrumb navigation. Excellent for photoblogs.\\nDownload Simplr template\\nView the Simplr README\\nLive preview the Simplr template\\n\\n\\nveryplaintxt\\n\\nThe original minimalist two-column theme for WordPress. A fluid layout with a newspaper-ish feel, its solid layout, thoughtful design elements, and numerous theme options make this theme much more than simply “plain.” This is the minimal classic WordPress theme.\\nDownload veryplaintxt template\\nView the veryplaintxt README\\nLive preview the veryplaintxt template\\n\\n\\nBack to contents\\n\\n\\nExperiments (Plugins)\\nI have produced a number of plugins to modify and enhance WordPress to fit the needs I have required of it at times. In the spirit of sharing, I offer these plugins as experiments for general consumption, each licensed with the GNU General Public License.\\nBy downloading content, you agree to the Terms of use.\\n\\n\\nBlog Summary\\nBlog Summary produces a semantic, hAtom-enabled list of latest blog entries with excerpts, dates, and comments links that is generated with the shortcode [blog-summary] on any post or page.\\nMore info and download from WordPress Extend\\n\\n\\nCustom Field Widget\\nCustom Field Widget displays values of custom field keys, allowing post- and page-specific meta sidebar content with limitless applications.\\nMore info and download from WordPress Extend\\n\\n\\n\\nExtended Options\\nExtended Options keeps meta data and certain WordPress tweaks persistent regardless of the active theme and without modifying any files.\\nMore info and download from WordPress Extend\\n\\n\\nBack to contents\\n\\n\\nMiscellaneous\\nMaybe you have questions. And maybe I have answers.\\nCode\\nIf you are interested in looking back at the development of these themes and plugins, then you are in luck. The experiments (i.e., plugins) are hosted in a Google Code repository called wp-plugin-dev. For themes, each has its own repository (see list below). You can also browse a folder here of downloads, too.\\n\\n\\nTheme Google Code Repositories\\n\\nBarthelme on Google Code\\nblog.txt on Google Code\\n on Google Code\\nSandbox on Google Code\\nSimplr on Google Code\\nveryplaintxt on Google Code\\n\\n\\n\\nContact\\nIf you have a question about any of the legacy WordPress software provided here, ask someone else. I no longer provide support. Sorry. However, the WordPress Support Forums are a great resource of information. Please go there. But if you really need to contact me for some non-support–related topic, you can do so via one of the sites mentioned below.\\nLinks\\nAs I mentioned, I am doing other things nowadays. You can find me on Twitter or perhaps my infrequently updated blog or, even better, check out some of my photography.\\nBack to contents\\n\\n\\nLicense\\nThe plaintxt.org themes and plugins, © 2004–2010 Scott Allan Wallick, are licensed under the GNU General Public License:\\n\\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\\nThis program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. See the GNU General Public License for more details.\\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc, 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.\\n\\nBack to contents\\n\\n\\nTerms of use\\nBy accessing and downloading content, you agree to these Terms of use.\\n\\n\\nAcceptance of terms.\\nYou agree to these terms by using this site. Your access to, and use of, this Web site (collectively referred to as the “Site”) is subject to the following Terms of Use and all applicable laws and regulations. By accessing and using the Site, you accept, without limitation or qualification, these Terms of Use, and acknowledge that any other agreements between you and plaintxt.org are superseded with respect to this subject matter. If you do not agree and accept, without limitation or qualification, these Terms of Use, you must exit the Site immediately, not use the files herein, and/or not subscribe to this service.\\n\\n\\nDisclaimer of warranties.\\nAll content is subject to change and is provided to you “as is” without any warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability, fitness for a particular purpose, or noninfringement. Please note that some jurisdictions may not allow the exclusion of implied warranties, so some of the above exclusions may not apply to you. Without limiting the foregoing, plaintxt.org neither warrants nor represents that your use of any Content will not infringe the rights of any third parties nor that the Content will be accurate, complete, or up-to-date. Additionally, with reference to any postings, comments, transmissions, and the like that may be on the Site, plaintxt.org assumes no responsibility or liability arising from any infringing, threatening, false, misleading, abusive, harassing, libelous, defamatory, vulgar, obscene, scandalous, inflammatory, pornographic, or profane material or any material that could constitute or encourage conduct that would be considered a criminal offense, give rise to civil liability, or otherwise violate any law, contained in any such locations on the Site.\\n\\n\\nExclusion of liability.\\nYour use of the Site is at your own risk. Neither plaintxt.org nor any of its sponsors, affiliates, participants, or visitors, nor any of its agents or any other party involved in creating, producing, or delivering the Site or its content, are liable for any direct, indirect, punitive, incidental, special, consequential, or other damages arising out of or in any way connected with the use of this Site or content, whether based on contract, tort, strict liability, or otherwise, even if advised of the possibility of such damages. Please note that some jurisdictions may not allow the exclusion of certain damages, so some of the above exclusions may not apply to you.\\n\\n\\nExclusion of indemnity.\\nBy using this Site you agree to indemnify and hold plaintxt.org, its subsidiaries, affiliates, successors, assigns, directors, officers, agents, service providers, suppliers, and employees harmless from any claim or demand, including reasonable attorney fees and court costs, made by any third party due to or arising out of Content you download, post, access, or make available through the Site, your use of the Site, your violation of these Terms of Use, your breach of any of the representations and warranties herein, or your violation of any rights of another.\\n\\n\\nThird-party disclaimer.\\nplaintxt.org may provide hyperlinks to third-party Web sites or other resources solely for your convenience. plaintxt.org does not control, and is not responsible for, the content or privacy policies on, or the security of, such Web sites. Without limiting the foregoing, plaintxt.org specifically disclaims any responsibility if such sites:\\n\\ninfringe any third party’s intellectual property rights;\\nare inaccurate, incomplete, or misleading;\\nare not merchantable or fit for a particular purpose;\\ndo not provide adequate security;\\ncontain viruses or other items of a destructive nature; or\\nare libelous or defamatory.\\n\\nNeither does plaintxt.org endorse the content nor any products or services available on such Web sites. If you visit such Web sites or establish a hyperlink to such Web sites or the Site, you do so at your own risk and without the permission or endorsement of plaintxt.org.\\n\\n\\nCopyright policy.\\nAny Content downloaded by you from this Web site, including, without limitation, any files, images incorporated in or generated by the software, and data accompanying the software (collectively referred to as the “Content”) are the sole property of the owner, Scott Allan Wallick, except as expressly licensed by the GNU General Public License, http://www.gnu.org/copyleft/gpl.html. Downloading the Content does not transfer title to the Content, or any intellectual property rights therein, to you.\\nThis Site contains copyrighted and licensed materials (collectively referred to as the “Content”) which are owned by plaintxt.org or its contributors. It is illegal to duplicate, download, or distribute any Content from this Site, except as expressly licensed and in accordance with the GNU General Public License, http://www.gnu.org/copyleft/gpl.html. You may not use photos or images of people or identifiable entities in any manner that suggests the endorsement or association of any product or service or in connection with any pornographic or immoral materials.\\n\\n\\nRevisions to these terms.\\nWe reserve the right, at our sole discretion, to change, modify or otherwise alter these terms and conditions at any time. Such modifications shall become effective immediately upon the posting thereof. You must review this agreement on a regular basis to keep yourself apprised of any changes.\\n\\n\\nBack to contents\\n\\n\\n\\nLast updated 15 October 2010\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web Loading\n",
    "# import bs4\n",
    "\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"http://www.plaintxt.org/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(class_=(\"class-names1\", \"class-names2\"))\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "loader = WebBaseLoader(\"http://www.plaintxt.org/\")\n",
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation (Feature Engineering)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(pdf_document)\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Vector DB\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "db = Chroma.from_documents(documents[:20],HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n"
     ]
    }
   ],
   "source": [
    "# Making queries from vector db\n",
    "query = \"What is Model Architecture?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db2 = FAISS.from_documents(documents[:20], HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n"
     ]
    }
   ],
   "source": [
    "result = db2.similarity_search(\"What is Model Architecture?\")\n",
    "print(result[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
